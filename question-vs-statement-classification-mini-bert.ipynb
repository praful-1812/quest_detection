{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-05-23T13:39:34.474663Z","iopub.status.busy":"2021-05-23T13:39:34.474316Z","iopub.status.idle":"2021-05-23T13:39:34.489176Z","shell.execute_reply":"2021-05-23T13:39:34.48822Z","shell.execute_reply.started":"2021-05-23T13:39:34.474633Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["./data/test.tsv\n","./data/dev.tsv\n","./data/train.tsv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('./data'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-05-23T13:39:34.629077Z","iopub.status.busy":"2021-05-23T13:39:34.628562Z","iopub.status.idle":"2021-05-23T13:39:35.437082Z","shell.execute_reply":"2021-05-23T13:39:35.436073Z","shell.execute_reply.started":"2021-05-23T13:39:34.629027Z"},"trusted":true},"outputs":[],"source":["# !git clone https://github.com/shahrukhx01/fine-tune-bert.git"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-05-23T13:39:35.440743Z","iopub.status.busy":"2021-05-23T13:39:35.440462Z","iopub.status.idle":"2021-05-23T13:39:35.449886Z","shell.execute_reply":"2021-05-23T13:39:35.448964Z","shell.execute_reply.started":"2021-05-23T13:39:35.440714Z"},"trusted":true},"outputs":[],"source":["# %cd fine-tune-bert/"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-05-23T13:48:43.322837Z","iopub.status.busy":"2021-05-23T13:48:43.322466Z","iopub.status.idle":"2021-05-23T13:48:43.326988Z","shell.execute_reply":"2021-05-23T13:48:43.326116Z","shell.execute_reply.started":"2021-05-23T13:48:43.322804Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-01-10 15:03:53.420275: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-01-10 15:03:53.761492: E tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay\n","2023-01-10 15:03:54.370559: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer.so.8'; dlerror: libnvinfer.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lb/cuda/lib64:/usr/local/cuda-11.2/lib64\n","2023-01-10 15:03:54.370648: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer_plugin.so.8'; dlerror: libnvinfer_plugin.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lb/cuda/lib64:/usr/local/cuda-11.2/lib64\n","2023-01-10 15:03:54.370658: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]}],"source":["import torch\n","from model import BERTClassifier\n","from config import BertOptimConfig\n","from train import train_model\n","from data_loader import QuestionsDataLoader\n","\n","    "]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-05-23T13:39:41.588469Z","iopub.status.busy":"2021-05-23T13:39:41.588192Z","iopub.status.idle":"2021-05-23T13:39:41.64158Z","shell.execute_reply":"2021-05-23T13:39:41.640659Z","shell.execute_reply.started":"2021-05-23T13:39:41.588443Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"\\ndata_path = {\\n    'train': '/kaggle/input/quora-question-keyword-pairs/train.tsv',\\n    'dev': '/kaggle/input/quora-question-keyword-pairs/dev.tsv',\\n    'test': '/kaggle/input/quora-question-keyword-pairs/test.tsv'\\n}\\ndata_loaders = QuestionsDataLoader(data_path, batch_size=8)\\nmodel = BERTClassifier(num_labels=num_labels).get_model()\\noptim_config = BertOptimConfig(model=model, train_dataloader=data_loaders.train_dataloader, epochs=epochs)\\n\\n## execute the training routine\\nmodel = train_model(model=model, \\n            optimizer=optim_config.optimizer, \\n            scheduler=optim_config.scheduler, \\n            train_dataloader=data_loaders.train_dataloader, \\n            validation_dataloader=data_loaders.validation_dataloader, \\n            epochs=epochs, device=device)\\n\""]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["epochs = 2\n","num_labels = 2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\"\"\"\n","data_path = {\n","    'train': '/kaggle/input/quora-question-keyword-pairs/train.tsv',\n","    'dev': '/kaggle/input/quora-question-keyword-pairs/dev.tsv',\n","    'test': '/kaggle/input/quora-question-keyword-pairs/test.tsv'\n","}\n","data_loaders = QuestionsDataLoader(data_path, batch_size=8)\n","model = BERTClassifier(num_labels=num_labels).get_model()\n","optim_config = BertOptimConfig(model=model, train_dataloader=data_loaders.train_dataloader, epochs=epochs)\n","\n","## execute the training routine\n","model = train_model(model=model, \n","            optimizer=optim_config.optimizer, \n","            scheduler=optim_config.scheduler, \n","            train_dataloader=data_loaders.train_dataloader, \n","            validation_dataloader=data_loaders.validation_dataloader, \n","            epochs=epochs, device=device)\n","\"\"\"\n","## test model performance on unseen test set\n","##eval_model(model=model, test_dataloader=data_loaders.test_dataloader, device=device)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-05-23T13:41:17.111909Z","iopub.status.busy":"2021-05-23T13:41:17.111566Z","iopub.status.idle":"2021-05-23T13:42:27.098725Z","shell.execute_reply":"2021-05-23T13:42:27.09777Z","shell.execute_reply.started":"2021-05-23T13:41:17.111874Z"},"trusted":true},"outputs":[],"source":["# !wget https://get.station307.com/gIfe63unMw4/query_classifier.pickle"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2021-05-23T13:42:27.101102Z","iopub.status.busy":"2021-05-23T13:42:27.100736Z","iopub.status.idle":"2021-05-23T13:42:27.222296Z","shell.execute_reply":"2021-05-23T13:42:27.221393Z","shell.execute_reply.started":"2021-05-23T13:42:27.101058Z"},"trusted":true},"outputs":[],"source":["import pickle\n","\"\"\"with open('query_classifier.pickle', 'wb') as handle:\n","    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\"\"\"\n","with open('query_classifier.pickle', 'rb') as handle:\n","    model = pickle.load(handle)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-05-23T13:51:48.980949Z","iopub.status.busy":"2021-05-23T13:51:48.980604Z","iopub.status.idle":"2021-05-23T13:51:48.994458Z","shell.execute_reply":"2021-05-23T13:51:48.993449Z","shell.execute_reply.started":"2021-05-23T13:51:48.980916Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import torch\n","from keras_preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertConfig\n","from tqdm import tqdm\n","import logging\n","\n","## setting the threshold of logger to INFO\n","logging.basicConfig(filename='data_loader.log', level=logging.INFO)\n","\n","## creating an object\n","logger = logging.getLogger()\n","  \n","\n","\n","class QuestionsData:\n","    def __init__(self, data_path, start_offset=0, end_offset=1000, max_sequence_length=512):\n","        \"\"\"\n","        Load dataset and bert tokenizer\n","        \"\"\"\n","        ## load data into memory\n","        self.test_df = pd.read_csv(data_path['test'], sep='\\t')[start_offset:end_offset]\n","        ## set max sequence length for model\n","        self.max_sequence_length = max_sequence_length\n","        ## get bert tokenizer\n","        self.tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-mini', do_lower_case=True)\n","    \n","    def train_val_test_split(self):\n","        \"\"\"\n","        Separate out labels and texts\n","        \"\"\"\n","        test_texts = self.test_df['query'].values\n","        test_labels = self.test_df.target.values\n","        \n","        return test_texts, test_labels\n","    \n","    def preprocess(self, texts):\n","        \"\"\"\n","        Add bert token (CLS and SEP) tokens to each sequence pre-tokenization\n","        \"\"\"\n","        ## separate labels and texts before preprocessing\n","        # Adding CLS and SEP tokens at the beginning and end of each sequence for BERT\n","        texts_processed = [\"[CLS] \" + str(sequence) + \" [SEP]\" for sequence in texts]\n","        return texts_processed\n","        \n","    def tokenize(self, texts):\n","        \"\"\"\n","        Use bert tokenizer to tokenize each sequence and post-process \n","        by padding or truncating to a fixed length\n","        \"\"\"\n","        ## tokenize sequence\n","        tokenized_texts = [self.tokenizer.tokenize(text) for text in (texts)]\n","        \n","        ## convert tokens to ids\n","        text_ids = [self.tokenizer.convert_tokens_to_ids(x) for x in (tokenized_texts)]\n","\n","        ## pad our text tokens for each sequence\n","        text_ids_post_processed = pad_sequences(text_ids, \n","                                       maxlen=self.max_sequence_length, \n","                                       dtype=\"long\", \n","                                       truncating=\"post\", \n","                                       padding=\"post\") \n","        return text_ids_post_processed\n","\n","    def create_attention_mask(self, text_ids):\n","        \"\"\"\n","        Add attention mask for padding tokens\n","        \"\"\"\n","        attention_masks = []\n","        # create a mask of 1s for each token followed by 0s for padding\n","        for seq in (text_ids):\n","            seq_mask = [float(i>0) for i in seq]\n","            attention_masks.append(seq_mask)\n","        return attention_masks\n","\n","    def process_texts(self):\n","        \"\"\"\n","        Apply preprocessing and tokenization pipeline of texts\n","        \"\"\"\n","        ## perform the split\n","        test_texts, test_labels = self.train_val_test_split()\n","\n","        ## preprocess train, val, test texts\n","        test_texts_processed = self.preprocess(test_texts)\n","        \n","        del test_texts\n","        \n","        ## preprocess train, val, test texts\n","        test_ids = self.tokenize(test_texts_processed)\n","\n","       \n","        del test_texts_processed \n","        \n","        ## create masks for train, val, test texts\n","        test_masks = self.create_attention_mask(test_ids)\n","        return (\n","                test_ids,\n","                test_masks,\n","                test_labels\n","                )\n","\n","    \n","    def text_to_tensors(self):\n","        \"\"\"\n","        Converting all the data into torch tensors\n","        \"\"\"\n","        test_ids, \\\n","        test_masks, \\\n","        test_labels = self.process_texts()\n","\n","        \n","        ## convert inputs, masks and labels to torch tensors\n","       \n","        self.test_inputs = torch.tensor(test_ids)\n","        self.test_labels = torch.tensor(test_labels)\n","        self.test_masks = torch.tensor(test_masks)\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-05-23T13:51:52.791861Z","iopub.status.busy":"2021-05-23T13:51:52.791524Z","iopub.status.idle":"2021-05-23T13:51:52.800907Z","shell.execute_reply":"2021-05-23T13:51:52.799886Z","shell.execute_reply.started":"2021-05-23T13:51:52.79183Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","class QuestionsDataLoader:\n","\n","    def __init__(self, data, data_file, batch_size=8):\n","        self.spam_data = data\n","        self.batch_size = batch_size\n","        self.create_loaders()\n","    \n","    def create_loaders(self):\n","        \"\"\"\n","        Create Torch dataloaders for data splits\n","        \"\"\"\n","        self.spam_data.text_to_tensors()\n","        \n","        \n","        test_data = TensorDataset(self.spam_data.test_inputs, \n","                                        self.spam_data.test_masks, \n","                                        self.spam_data.test_labels)\n","        test_sampler = SequentialSampler(test_data)\n","        self.test_dataloader = DataLoader(test_data, \n","                                                sampler=test_sampler, \n","                                                batch_size=self.batch_size)\n","        "]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2021-05-23T13:51:52.954078Z","iopub.status.busy":"2021-05-23T13:51:52.953738Z","iopub.status.idle":"2021-05-23T13:51:52.963379Z","shell.execute_reply":"2021-05-23T13:51:52.962365Z","shell.execute_reply.started":"2021-05-23T13:51:52.954046Z"},"trusted":true},"outputs":[],"source":["import torch\n","from utils import flat_accuracy\n","\n","def eval_model(model, test_dataloader, device):\n","    ## tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    ## evaluate data for one epoch\n","    for batch in test_dataloader:\n","        ## add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        ## unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        ## avoiding model's computation and storage of gradients -> saving memory and speeding up validation\n","        with torch.no_grad():\n","            # forward pass, calculate logit predictions\n","            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","        \n","        ## move logits and labels to CPU\n","        logits = logits['logits'].detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","    return eval_accuracy/nb_eval_steps"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2021-05-23T13:59:10.488937Z","iopub.status.busy":"2021-05-23T13:59:10.488599Z","iopub.status.idle":"2021-05-23T15:26:26.979952Z","shell.execute_reply":"2021-05-23T15:26:26.979045Z","shell.execute_reply.started":"2021-05-23T13:59:10.488905Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  1%|          | 1/155 [00:15<38:59, 15.19s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/lb/praful/quest_detect/fine-tune-bert/question-vs-statement-classification-mini-bert.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lb/praful/quest_detect/fine-tune-bert/question-vs-statement-classification-mini-bert.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m accuracy \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lb/praful/quest_detect/fine-tune-bert/question-vs-statement-classification-mini-bert.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(outer_batch_size, \u001b[39m1557858\u001b[39m, outer_batch_size)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lb/praful/quest_detect/fine-tune-bert/question-vs-statement-classification-mini-bert.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m## batching test set\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lb/praful/quest_detect/fine-tune-bert/question-vs-statement-classification-mini-bert.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     data \u001b[39m=\u001b[39m QuestionsData(data_path, start_offset\u001b[39m=\u001b[39;49mi\u001b[39m-\u001b[39;49mouter_batch_size, end_offset\u001b[39m=\u001b[39;49mi)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lb/praful/quest_detect/fine-tune-bert/question-vs-statement-classification-mini-bert.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     quest_loader \u001b[39m=\u001b[39m QuestionsDataLoader(data, data_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lb/praful/quest_detect/fine-tune-bert/question-vs-statement-classification-mini-bert.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     quest_loader\u001b[39m.\u001b[39mcreate_loaders()\n","\u001b[1;32m/home/lb/praful/quest_detect/fine-tune-bert/question-vs-statement-classification-mini-bert.ipynb Cell 11\u001b[0m in \u001b[0;36mQuestionsData.__init__\u001b[0;34m(self, data_path, start_offset, end_offset, max_sequence_length)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lb/praful/quest_detect/fine-tune-bert/question-vs-statement-classification-mini-bert.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_sequence_length \u001b[39m=\u001b[39m max_sequence_length\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lb/praful/quest_detect/fine-tune-bert/question-vs-statement-classification-mini-bert.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m## get bert tokenizer\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lb/praful/quest_detect/fine-tune-bert/question-vs-statement-classification-mini-bert.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mprajjwal1/bert-mini\u001b[39;49m\u001b[39m'\u001b[39;49m, do_lower_case\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1744\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1744\u001b[0m         resolved_vocab_files[file_id] \u001b[39m=\u001b[39m cached_path(\n\u001b[1;32m   1745\u001b[0m             file_path,\n\u001b[1;32m   1746\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1747\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1748\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1749\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   1750\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1751\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1752\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1753\u001b[0m         )\n\u001b[1;32m   1755\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m   1756\u001b[0m         \u001b[39mif\u001b[39;00m local_files_only:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/hub.py:284\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    280\u001b[0m     local_files_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    283\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m    285\u001b[0m         url_or_filename,\n\u001b[1;32m    286\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    287\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    288\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    289\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    290\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    291\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    292\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    293\u001b[0m     )\n\u001b[1;32m    294\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    295\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/hub.py:501\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m local_files_only:\n\u001b[1;32m    500\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 501\u001b[0m         r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mhead(url, headers\u001b[39m=\u001b[39;49mheaders, allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, proxies\u001b[39m=\u001b[39;49mproxies, timeout\u001b[39m=\u001b[39;49metag_timeout)\n\u001b[1;32m    502\u001b[0m         _raise_for_status(r)\n\u001b[1;32m    503\u001b[0m         etag \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mX-Linked-Etag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mETag\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/api.py:100\u001b[0m, in \u001b[0;36mhead\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a HEAD request.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 100\u001b[0m \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mhead\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py:1040\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1043\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1044\u001b[0m         (\n\u001b[1;32m   1045\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1051\u001b[0m     )\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    359\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[1;32m    360\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39msocket_options\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket_options\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m source_address:\n\u001b[1;32m     84\u001b[0m         sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m     sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     86\u001b[0m     \u001b[39mreturn\u001b[39;00m sock\n\u001b[1;32m     88\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39merror \u001b[39mas\u001b[39;00m e:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["if __name__=='__main__':\n","    data_path = {\n","    'train': './data/train.tsv',\n","    'dev': './data/dev.tsv',\n","    'test': './data/test.tsv'\n","    }\n","    outer_batch_size = 10000\n","    accuracy = []\n","    for i in tqdm(range(outer_batch_size, 1557858, outer_batch_size)):\n","        ## batching test set\n","        data = QuestionsData(data_path, start_offset=i-outer_batch_size, end_offset=i)\n","        quest_loader = QuestionsDataLoader(data, data_path)\n","        quest_loader.create_loaders()\n","        # accuracy.append(eval_model(model=model, test_dataloader=quest_loader.test_dataloader, device=device))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-23T15:26:26.982126Z","iopub.status.busy":"2021-05-23T15:26:26.98176Z","iopub.status.idle":"2021-05-23T15:26:26.988476Z","shell.execute_reply":"2021-05-23T15:26:26.987376Z","shell.execute_reply.started":"2021-05-23T15:26:26.982085Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","np.mean(accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":4}
